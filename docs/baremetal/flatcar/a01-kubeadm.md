Tips to manage your k8s cluster (kubeadm)

# Deploy CNI on the control plane
At this point you should be able to:

* SSH into the machine
`ssh core@<<node ip address>>`

* Deploy a CNI
This project uses Calico (v3.24.1) [as of Jan 27, 2024]
`kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml`

# Certificates
https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/

Can be externally managed but are internally generated by default.
In this project, certificates are internally managed.

Certificates expire in one year when generated. Check expiration with:
```
kubeadm certs check-expiration
```

https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#automatic-certificate-renewal
Certificates are automatically renewed on control plane upgrade (which is recommended at least once a year). Otherwise, manual certificate is available.

https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#manual-certificate-renewal
Manual cert renewal:
```
kubeadm certs renew all

# Update kubeconfig with new certs
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

# Creating a kubeconfig for clients
https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubeconfig-additional-users
Retrieve the static ip for your control plane cluster. This static ip was the one used in the flatcar.md setup

```
# kubeadm-user.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
# Will be used as the target "cluster" in the kubeconfig
clusterName: "kubernetes"
# Will be used as the "server" (IP or DNS name) of this cluster in the kubeconfig
controlPlaneEndpoint: "<<static ip of control plane, see flatcar.md#configure-a-wired-network for details>>:6443"
# The cluster CA key and certificate will be loaded from this local directory
certificatesDir: "/etc/kubernetes/pki"
```

Generate the kubeconfig (it'll print in stdout)
`sudo kubeadm kubeconfig user --config kubeadm-user.yaml --client-name admin`

At this point you can use this .kube/config to access this cluster with user `admin`

# Adding permissions to the user
In this project, we use ClusterRole and ClusterRoleBinding to allow read/write access to all resources in all namespaces.

You can find the configurations in `kubectl-admin-binding.yaml` and `kubectl-admin-role.yaml`.
The `admin` user is hard-coded in `kubectl-admin-binding.yaml`.

On the control plane host, add the role and binding:
`kubectl apply -f kubectl-admin-binding.yaml`
`kubectl apply -f kubectl-admin-role.yaml`

At this point your user should be able to run `kubectl get nodes` on the client

# Verify that the cgroupdriver is systemd and other settings
`kubectl describe cm kubelet-config -n kube-system`

# Storage
There are two ways this project will provision peristant volumes: local node storage or NAS (future). As of this comment, only instructions for local storage provisioning are provided.

## NAS
TBD

## Local Storage
https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/getting-started.md#option-3-baremetal-environments
https://kubernetes.io/docs/concepts/storage/volumes/#local
https://kubernetes.io/docs/concepts/storage/storage-classes/#local
https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/

Prerequisites: Your drive should be partitioned for the volumes you want to provision. If you've done the optional partitioning step in `a00-flatcar.md` then you have fulfilled this prerequisite.



# Upgrades
https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

# Single-node k8s clusters
By default, `kubeadm` will assign a `node-role.kubernetes.io/control-plane:NoSchedule` taint on your node. If you're running a single node k8s cluster then you will not be able to schedule any workloads on that node. This is intended as running workloads on your control-plane is not recommended. However, if you do not have the resources for more nodes, then you can remove the taint to schedule workloads on the control plane by running:

```
kubectl taint nodes <nodename> node-role.kubernetes.io/control-plane:NoSchedule-
```