Tips to manage your k8s cluster (kubeadm)

# Deploy CNI on the control plane
At this point you should be able to:

* SSH into the machine
`ssh core@<<node ip address>>`

* Deploy a CNI
This project uses Calico (v3.24.1) [as of Jan 27, 2024]
`kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml`

# Certificates
https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/

Can be externally managed but are internally generated by default.
In this project, certificates are internally managed.

Certificates expire in one year when generated. Check expiration with:
```
kubeadm certs check-expiration
```

https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#automatic-certificate-renewal
Certificates are automatically renewed on control plane upgrade (which is recommended at least once a year). Otherwise, manual certificate is available.

https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#manual-certificate-renewal
Manual cert renewal:
```
kubeadm certs renew all

# Update kubeconfig with new certs
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

# Creating a kubeconfig for clients
https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubeconfig-additional-users
Retrieve the static ip for your control plane cluster. This static ip was the one used in the flatcar.md setup

```
# kubeadm-user.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
# Will be used as the target "cluster" in the kubeconfig
clusterName: "kubernetes"
# Will be used as the "server" (IP or DNS name) of this cluster in the kubeconfig
controlPlaneEndpoint: "<<static ip of control plane, see flatcar.md#configure-a-wired-network for details>>:6443"
# The cluster CA key and certificate will be loaded from this local directory
certificatesDir: "/etc/kubernetes/pki"
```

Retrieve the config from this project (with the modifications you've made above)
`wget "https://raw.githubusercontent.com/toiletpapar/smithers-infrastructure/main/docs/baremetal/flatcar/kubeadm-user.yaml"`

Generate the kubeconfig (it'll print in stdout)
`sudo kubeadm kubeconfig user --config kubeadm-user.yaml --client-name admin`

At this point you can use this .kube/config to access this cluster with user `admin` (but you won't have any permissions)

# Adding permissions to the user
In this project, we use ClusterRole and ClusterRoleBinding to allow read/write access to all resources in all namespaces.

You can find the configurations in `kubectl-admin-binding.yaml` and `kubectl-admin-role.yaml`.
The `admin` user is hard-coded in `kubectl-admin-binding.yaml`.

On the control plane host, add the role and binding:
`wget "https://raw.githubusercontent.com/toiletpapar/smithers-infrastructure/main/docs/baremetal/flatcar/kubectl-admin-binding.yaml"`
`kubectl apply -f kubectl-admin-binding.yaml`
`wget "https://raw.githubusercontent.com/toiletpapar/smithers-infrastructure/main/docs/baremetal/flatcar/kubectl-admin-role.yaml"`
`kubectl apply -f kubectl-admin-role.yaml`

At this point your user should be able to run `kubectl get nodes` on the client

# Using Helm
This project will assume a local helm installation going forward. Helm is a Kubernetes package manager. Get more details about helm here:
https://helm.sh/docs/intro/install/
https://helm.sh/docs/intro/using_helm/

# Verify that the cgroupdriver is systemd and other settings
`kubectl describe cm kubelet-config -n kube-system`

# Storage
There are two ways this project will provision peristant volumes: local node storage or NAS (future). As of this comment, only instructions for local storage provisioning are provided.

## NAS
TBD

## Local Storage
https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/getting-started.md
https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/operations.md#create-a-directory-for-provisioner-discovering
https://kubernetes.io/docs/concepts/storage/volumes/#local
https://kubernetes.io/docs/concepts/storage/storage-classes/#local
https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/

Prerequisites: Your drive should be partitioned for the volumes you want to provision (this provides capacity isolation). If you've done the optional partitioning step in `a00-flatcar.md` then you have fulfilled this prerequisite.

We'll use the local static provisioner to help manage the lifecycle of local persistant volumes.

Create a StorageClass with `volumeBindingMode` set to `WaitForFirstConsumer` to delay volume binding until pod scheduling to handle multiple local PVs in a single pod.
`kubectl create -f docs/baremetal/flatcar/fast-disks-data.yaml`

This project has customized the local provisioner at `docs/baremetal/flatcar/local-static-provision-values.yaml`. At a high-level, the following customizations were made:
* Added a storage class that specifies which partition to use and the mount path

Add local static provison repo to helm:
`helm repo add sig-storage-local-static-provisioner https://kubernetes-sigs.github.io/sig-storage-local-static-provisioner`

Install
`helm install sig-storage-local-static-provisioner/local-static-provisioner -f ./docs/baremetal/flatcar/local-static-provisioner-values.yaml`

Verify that local-volume-provisioner and the `fast-disks-data` pv were created successfully
`helm list -A`
`kubectl get pv`

# Upgrades
https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

# Single-node k8s clusters
By default, `kubeadm` will assign a `node-role.kubernetes.io/control-plane:NoSchedule` taint on your node. If you're running a single node k8s cluster then you will not be able to schedule any workloads on that node. This is intended as running workloads on your control-plane is not recommended. However, if you do not have the resources for more nodes, then you can remove the taint to schedule workloads on the control plane by running:

```
kubectl taint nodes <nodename> node-role.kubernetes.io/control-plane:NoSchedule-
```