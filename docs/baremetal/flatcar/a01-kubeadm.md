Tips to manage your k8s cluster (kubeadm)

# Certificates
https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/

Can be externally managed but are internally generated by default.
In this project, certificates are internally managed.

Certificates expire in one year when generated. Check expiration with:
```
kubeadm certs check-expiration
```

https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#automatic-certificate-renewal
Certificates are automatically renewed on control plane upgrade (which is recommended at least once a year). Otherwise, manual certificate is available.

https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#manual-certificate-renewal
Manual cert renewal:
```
kubeadm certs renew all

# Update kubeconfig with new certs
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

# Creating a kubeconfig for clients
https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubeconfig-additional-users
Retrieve the static ip for your control plane cluster. This static ip was the one used in the flatcar.md setup

```
# kubeadm-user.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
# Will be used as the target "cluster" in the kubeconfig
clusterName: "kubernetes"
# Will be used as the "server" (IP or DNS name) of this cluster in the kubeconfig
controlPlaneEndpoint: "<<static ip of control plane, see flatcar.md#configure-a-wired-network for details>>:6443"
# The cluster CA key and certificate will be loaded from this local directory
certificatesDir: "/etc/kubernetes/pki"
```

Generate the kubeconfig (it'll print in stdout)
`sudo kubeadm kubeconfig user --config kubeadm-user.yaml --client-name admin`

At this point you can use this .kube/config to access this cluster with user `admin`

# Adding permissions to the user
In this project, we use ClusterRole and ClusterRoleBinding to allow read/write access to all resources in all namespaces.

You can find the configurations in `kubectl-admin-binding.yaml` and `kubectl-admin-role.yaml`.
The `admin` user is hard-coded in `kubectl-admin-binding.yaml`.

On the control plane host, add the role and binding:
`kubectl apply -f kubectl-admin-binding.yaml`
`kubectl apply -f kubectl-admin-role.yaml`

At this point your user should be able to run `kubectl get nodes` on the client

# Verify systemd cgroupdriver and other settings
`kubectl describe cm kubelet-config -n kube-system`

# Upgrades
https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

# Single-node k8s clusters
By default, `kubeadm` will assign a `node-role.kubernetes.io/control-plane:NoSchedule` taint on your node. If you're running a single node k8s cluster then you will not be able to schedule any workloads on that node. This is intended as running workloads on your control-plane is not recommended. However, if you do not have the resources for more nodes, then you can remove the taint to schedule workloads on the control plane by running:

```
kubectl taint nodes <nodename> node-role.kubernetes.io/control-plane:NoSchedule-
```

# Pulling from a private registry
This project uses `registry.smithers.private` as the private registry for all project images. See `pi/b01-pi-regstiry.md` for details. As a result, you'll need to trust the certificate issued by `registry.smithers.private`.

```
# shell on kubeadm node
scp core@registry.smithers.private:/home/core/certs/domain.crt domain.crt
cp domain.crt /etc/docker/certs.d/registry.smithers.private/ca.crt
```